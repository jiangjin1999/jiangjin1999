
### Q1：卷积神经网络和神经网络分别是什么？


- DNN是指深度神经网络，它是一个很广的概念，某种意义上CNN、RNN、GAN等都属于其范畴之内。
- DNN与CNN（卷积神经网络）的区别是DNN特指全连接的神经元结构，并不包含卷积单元或是时间上的关联。
- DNN是指包含多个隐层的神经网络，根据神经元的特点，可以分为MLP、CNNs、RNNs等。
- 从神经元的角度来讲解，MLP是最朴素的DNN，CNNs是encode了空间相关性的DNN，RNNs是encode进了时间相关性的DNN

#### 为什么会从神经网络到卷积神经网络？
发展：
1. 感知机（Rosenblatt）：拥有输入层、输出层和一个隐含层。输入的特征向量通过隐含层变换达到输出层，在输出层得到分类结果。通俗看来，他就是一个简单的线性分类器（线性回归+激活函数）；形式简单，所以对复杂函数表示很难。能解决的问题-线性可分的问题（与或非）


2. 多层感知机（Rumelhart、Williams、Hinton、LeCun等人）：多个隐含层的感知机。可以表示更复杂的函数（非线性的）
带来问题：过拟合-梯度消失
（本质是训练问题，如何理解？想想最小二乘方法，如何在神经网络中找到这个可以趋于最优的这个网络即他的参数-一个寻找函数最小值的问题）
-梯度爆炸和消失：梯度消失和梯度爆炸本质上是一样的，都是因为网络层数太深而引发的梯度反向传播中的连乘效应。
-解决方法：
- 换用Relu、LeakyRelu、Elu等激活函数
- BatchNormalization
- ResNet残差结构
-  LSTM结构
- 预训练加finetunning
- 梯度剪切、正则


3. 卷积神经网络
我们看到全连接DNN的结构里下层神经元和所有上层神经元都能够形成连接，带来的潜在问题是参数数量的膨胀。
假设输入的是一幅像素为1K*1K的图像，隐含层有1M个节点，光这一层就有10^12个权重需要训练，这不仅容易过拟合，而且极容易陷入局部最优。
另外，图像中有固有的局部模式（比如轮廓、边界，人的眼睛、鼻子、嘴等）可以利用，显然应该将图像处理中的概念和神经网络技术相结合。
此时我们可以祭出题主所说的卷积神经网络CNN。对于CNN来说，并不是所有上下层神经元都能直接相连，而是通过“卷积核”作为中介。
同一个卷积核在所有图像内是共享的，图像通过卷积操作后仍然保留原先的位置关系。

[参考资料](https://www.zhihu.com/question/29366638)


## 卷积神经网络
#### 卷积神经网络的基本概念
- 局部感受野（local receptive fields）
- 共享权重（shared weights）
- 池化（pooling）

#### 卷积神经网络的构成
典型的卷积神经网络由3部分构成：

- 卷积层
- 池化层
- 全连接层
卷积层负责提取图像中的局部特征；池化层用来大幅降低参数量级(降维)；全连接层类似传统神经网络的部分，用来输出想要的结果。

### 二维卷积层
一个最基础的卷积层，二维卷积层，基于长宽的特征提取。

### 填充和步幅
增大或者减小卷积后的图片的大小

### 多输入通道和多输出通道
- 使用多通道可以拓展卷积层的模型参数。
- 假设将通道维当作特征维，将高和宽维度上的元素当成数据样本，那么 1×1 卷积层的作用与全连接层等价。
- 1×1 卷积层通常用来调整网络层之间的通道数，并控制模型复杂度
### 池化层
- 最大池化和平均池化分别取池化窗口中输入元素的最大值和平均值作为输出。
- 池化层的一个主要作用是缓解卷积层对位置的过度敏感性。

### 卷积神经网络（LeNet)
#### 为什么用卷积？
1. 只用全连接像素离得比较远。
2. 图像比较大，参数太多。
卷积可以比较好的解决这两个问题
可以让图像保持形状，可以让图像规模变小。

#### LeNet(Yann LeCun)
LeNet交替使用卷积层和最大池化层后接全连接层来进行图像分

### 深度卷积神经网路（AlexNet）
特征应学习得到
在多层神经网络中，图像的第一级的表示可以是在特定的位置和⻆度是否出现边缘；
而第二级的表示说不定能够将这些边缘组合出有趣的模式，如花纹；
在第三级的表示中，也许上一级的花纹能进一步汇合成对应物体特定部位的模式。

- 缺失要素一：数据---ImageNet
- 缺失要素二：硬件---GPU

#### AlexNet（Alex Krizhevsky）
AlexNet与LeNet的设计理念非常相似，但也有显著的区别。
1. 与相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层。
2. AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数。
3. AlexNet通过丢弃法来控制全连接层的模型复杂度。（一种缓解过拟合的方法，就是把网络中隐层中的一部分神经元去掉）
4. AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合。

### 使用重复元素的网络（VGG）
VGG，它的名字来源于论文作者所在的实验室Visual Geometry Group。VGG提出了可以通过重复使用简单的基础块来构建深度模型的思路。

#### VGG块
VGG块的组成规律是：连续使用数个相同的填充为1、窗口形状为 3×3 的卷积层后接上一个步幅为2、窗口形状为 2×2 的最大池化层。卷积层保持输入的高和宽不变，而池化层则对其减半。

#### VGG 网络
现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。第一块的输出通道是64，之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。

### 网络中的网络（NiN ）
前几节介绍的LeNet、AlexNet和VGG在设计上的共同之处是：先以由卷积层构成的模块充分抽取空间特征，再以由全连接层构成的模块来输出分类结果。
其中，AlexNet和VGG对LeNet的改进主要在于如何对这两个模块加宽（增加通道数）和加深。
这里我们介绍网络中的网络（NiN）。它提出了另外一个思路，即串联多个由卷积层和“全连接”层构成的小网络来构建一个深层网络。

#### NiN块
卷积层的输入和输出通常是四维数组（样本，通道，高，宽），而全连接层的输入和输出则通常是二维数组（样本，特征）
![在这里插入图片描述](https://img-blog.csdnimg.cn/20210226223105271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80MzQ4MTMyMA==,size_16,color_FFFFFF,t_70)
- NiN重复使用由卷积层和代替全连接层的 1×1 卷积层构成的NiN块来构建深层网络。
- NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。
- NiN的以上设计思想影响了后面一系列卷积神经网络的设计。

### 含并行连结的网络（GoogLeNet）
改进

### ！ 批量归一化！
批量归一化的提出正是为了应对深度模型训练的挑战。在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。批量归一化和下一节将要介绍的残差网络为训练和设计深度模型提供了两类重要思路。

- 在模型训练时，批量归一化利用小批量上的均值和标准差，不断调整神经网络的中间输出，从而使整个神经网络在各层的中间输出的数值更稳定。
- 对全连接层和卷积层做批量归一化的方法稍有不同。
- 批量归一化层和丢弃层一样，在训练模式和预测模式的计算结果是不一样的。
### ！！ 残差网络！！

## 思考
1. 图像为什么是连续的，是因为，他在计算机中的表示是像素值的大小，这个值连续的，同时后面的卷积等表示也都是基于这一基础去进行提取特征的。
这样自然有疑问，一定要这样表示吗？自然语言又是如何表示的呢？
